{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download pt_core_news_lg\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import stardog\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import requests\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import json\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Stardog connection details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stardog variables\n",
    "STARDOG_ENDPOINT = os.getenv('STARDOG_ENDPOINT')\n",
    "STARDOG_USERNAME = os.getenv(\"STARDOG_USERNAME\")\n",
    "STARDOG_PASSWORD = os.getenv(\"STARDOG_PASSWORD\")\n",
    "\n",
    "connection_details = {\n",
    "    'endpoint': STARDOG_ENDPOINT,\n",
    "    'username': STARDOG_USERNAME,\n",
    "    'password': STARDOG_PASSWORD\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'IndigenousSlavery'\n",
    "conn = stardog.Connection(database_name, **connection_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [place]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query que busca as teses e os seus abstracts. Foi incluído um filtro para buscar apenas \n",
    "# os abstract em português, inglês ou em outra língua\n",
    "query = \"\"\"\n",
    "SELECT ?thesis ?abstract (lang(?abstract) AS ?lang) WHERE {\n",
    "  ?thesis a <http://purl.org/ontology/bibo/Thesis>.\n",
    "  ?thesis <http://purl.org/ontology/bibo/abstract> ?abstract.\n",
    "  FILTER (lang(?abstract) IN(\"pt\", \"en\"))\n",
    "  \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query2 = \"\"\"\n",
    "SELECT ?thesis ?ent  WHERE {\n",
    "  ?thesis a <http://purl.org/ontology/bibo/Thesis>.\n",
    "  ?thesis <https://schema.org/mentions> ?ent.  \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query3 = \"\"\"\n",
    "SELECT ?place   WHERE {\n",
    "  ?place a <https://schema.org/Place>. \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "# FILTER (langMatches(lang(?abstract),\"pt\"))\n",
    "# FILTER (langMatches(lang(?abstract),\"en\"))\n",
    "# FILTER (lang(?abstract) NOT IN(\"pt\", \"en\"))\n",
    "# LIMIT 20\n",
    "\n",
    "csv_results = conn.select(query3, content_type='text/csv')\n",
    "thesis_abstract = pd.read_csv(io.BytesIO(csv_results))\n",
    "thesis_abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499  teses processadas.\n",
      "999  teses processadas.\n",
      "1499  teses processadas.\n",
      "1999  teses processadas.\n",
      "2499  teses processadas.\n",
      "2999  teses processadas.\n",
      "3499  teses processadas.\n",
      "3999  teses processadas.\n",
      "4499  teses processadas.\n",
      "4999  teses processadas.\n",
      "5499  teses processadas.\n",
      "5999  teses processadas.\n",
      "6499  teses processadas.\n",
      "6999  teses processadas.\n",
      "7499  teses processadas.\n",
      "7999  teses processadas.\n",
      "8499  teses processadas.\n",
      "8999  teses processadas.\n",
      "9499  teses processadas.\n",
      "9999  teses processadas.\n",
      "10499  teses processadas.\n",
      "10999  teses processadas.\n",
      "11499  teses processadas.\n",
      "11999  teses processadas.\n",
      "12499  teses processadas.\n",
      "12999  teses processadas.\n",
      "13499  teses processadas.\n"
     ]
    }
   ],
   "source": [
    "# Extraíndo as entidades dos abstracts\n",
    "\n",
    "#Carregando os modelos SpaCy para inglês e português \n",
    "nlp_en = spacy.load(\"en_core_web_lg\")\n",
    "nlp_pt = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "#Dicionário que receberá as instâncias de cada tese\n",
    "instances_dic = {}\n",
    "\n",
    "for n in range(len(thesis_abstract)):\n",
    "\n",
    "    # Processando os abstracts em português\n",
    "    if thesis_abstract['lang'][n] == 'pt':\n",
    "\n",
    "        doc_pt = nlp_pt(str(thesis_abstract['abstract'][n]))\n",
    "        persons = []\n",
    "        gpes = []\n",
    "        #orgs = []\n",
    "        for ent in doc_pt.ents:\n",
    "            if ent.label_ == \"PER\": #\"PERSON\":\n",
    "                persons.append(ent)\n",
    "            if ent.label_ == \"LOC\": #\"GPE\":\n",
    "                gpes.append(ent)\n",
    "            #if ent.label_ == \"ORG\":\n",
    "            #    orgs.append(ent)\n",
    "\n",
    "        #instances_dic[thesis_abstract['thesis'][n].replace('tag:stardog:api:','')] = {'PER':persons, 'LOC':gpes, 'ORG':orgs, 'lang': 'pt'}\n",
    "        instances_dic[thesis_abstract['thesis'][n].replace('tag:stardog:api:','')] = {'PER':persons, 'LOC':gpes, 'lang': 'pt'}\n",
    "\n",
    "    # Processando os abstracts em inglês\n",
    "    if thesis_abstract['lang'][n] == 'en':\n",
    "\n",
    "        doc_en = nlp_en(str(thesis_abstract['abstract'][n]))\n",
    "        persons = []\n",
    "        gpes = []\n",
    "        #orgs = []\n",
    "        for ent in doc_en.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                persons.append(ent)\n",
    "            if ent.label_ == \"GPE\":\n",
    "                gpes.append(ent)\n",
    "            #if ent.label_ == \"ORG\":\n",
    "            #    orgs.append(ent)\n",
    "\n",
    "        #instances_dic[thesis_abstract['thesis'][n].replace('tag:stardog:api:','')] = {'PER':persons, 'LOC':gpes, 'ORG':orgs, 'lang': 'en'}\n",
    "        instances_dic[thesis_abstract['thesis'][n].replace('tag:stardog:api:','')] = {'PER':persons, 'LOC':gpes, 'lang': 'en'}\n",
    "\n",
    "    if n%500 == 499:\n",
    "        print(n, \" teses processadas.\")\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rascunho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcão que recebe uma string e a limpar para ficar no formato aceitável para uma URI\n",
    "def process_uri(x):\n",
    "    return (re.sub('[^a-zA-Z0-9_ ]', '',\n",
    "            unidecode(x.strip())\n",
    "                        .replace(\" \", \"_\")\n",
    "                        .replace(\"[\",\"\")\n",
    "                        .replace(\"]\",\"\")\n",
    "                        .replace(\"?\",\"\")\n",
    "                        .replace(\"'\",\"\")\n",
    "                        .lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando URIs para as entidades\n",
    "for thes in instances_dic:\n",
    "    for type_ent in ['LOC', 'PER']:\n",
    "        list_ent = []\n",
    "        for span in instances_dic[thes][type_ent]:\n",
    "            list_ent.append(process_uri(span.text))\n",
    "        instances_dic[thes][type_ent + '_uri'] = list(set(list_ent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparando o texto em turtle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixos\n",
    "prefixos = \"\"\" @prefix ns: <http://www.w3.org/2003/06/sw-vocab-status/ns#> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix xml: <http://www.w3.org/XML/1998/namespace> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix bibo: <http://purl.org/ontology/bibo/> .\n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\n",
    "@prefix vann: <http://purl.org/vocab/vann/> .\n",
    "@prefix event: <http://purl.org/NET/c4dm/event.owl#> .\n",
    "@prefix prism: <http://prismstandard.org/namespaces/1.2/basic/> .\n",
    "@prefix terms: <http://purl.org/dc/terms/> .\n",
    "@prefix schema: <http://schemas.talis.com/2005/address/schema#> .\n",
    "@prefix status: <http://purl.org/ontology/bibo/status/> .\n",
    "@prefix degrees: <http://purl.org/ontology/bibo/degrees/> .\n",
    "@prefix stardog: <tag:stardog:api:> .\n",
    "@base <http://www.w3.org/2002/07/owl#> .\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conectando com o banco e adicionando as triplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcão que recebe os prefixos e triplas e as carrega à base de dados\n",
    "def add_triplas_to_stardog(prefixos, triplas):\n",
    "\n",
    "    # Incluindo prefixos às triplas\n",
    "    triplas = prefixos + \" \" + triplas\n",
    "\n",
    "    ### Connect to the Stardog database\n",
    "    database_name = 'IndigenousSlavery'\n",
    "    conn = stardog.Connection(database_name, **connection_details)\n",
    "\n",
    "    conn.begin()\n",
    "    conn.add(stardog.content.Raw(triplas, 'text/turtle'))\n",
    "    conn.commit() # commit the transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparando as triplas para conectar entidades às teses e carregando à base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_uri = []\n",
    "for thes in instances_dic:\n",
    "    for loc in instances_dic[thes]['LOC_uri']:\n",
    "        loc_uri.append(loc)\n",
    "\n",
    "triplas = \"\"\" \"\"\"\n",
    "for loc in (set(loc_uri)):\n",
    "    triplas = triplas + \" \" + \"\"\"\n",
    "    stardog:\"\"\" + loc + \"\"\" rdf:type <https://schema.org/Place>.\"\"\"\n",
    "\n",
    "add_triplas_to_stardog(prefixos, triplas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_uri = []\n",
    "for thes in instances_dic:\n",
    "    for per in instances_dic[thes]['PER_uri']:\n",
    "        per_uri.append(per)\n",
    "\n",
    "triplas = \"\"\" \"\"\"\n",
    "for per in (set(per_uri)):\n",
    "    triplas = triplas + \" \" + \"\"\"\n",
    "    stardog:\"\"\" + per + \"\"\" rdf:type foaf:Person.\"\"\"\n",
    "\n",
    "add_triplas_to_stardog(prefixos, triplas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplas = \"\"\" \"\"\"\n",
    "n = 0\n",
    "for thes in instances_dic:\n",
    "    n = n + 1\n",
    "    for loc in instances_dic[thes]['LOC_uri']:\n",
    "        tripla = \"\"\"\n",
    "        stardog:\"\"\" + thes + \"\"\" <https://schema.org/mentions> stardog:\"\"\" + loc + \"\"\".\n",
    "        \"\"\"\n",
    "        triplas = triplas + \" \" + tripla\n",
    "\n",
    "    for per in instances_dic[thes]['PER_uri']:\n",
    "        tripla = \"\"\"\n",
    "        stardog:\"\"\" + thes + \"\"\" <https://schema.org/mentions> stardog:\"\"\" + per + \"\"\".\n",
    "        \"\"\"  \n",
    "        triplas = triplas + \" \" + tripla\n",
    "    \n",
    "add_triplas_to_stardog(prefixos, triplas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fim do rascunho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para uma lista de entidades verificar se são a mesma instância e qual o termo mais comum para representá-las\n",
    "\n",
    "def entidades_consolidadas(list_ent, th):\n",
    "    \n",
    "    if list_ent == []:\n",
    "        return None\n",
    "\n",
    "    # lista com os vetores das entidades\n",
    "  \n",
    "    list_vectors = np.array([i.vector for i in list_ent])\n",
    "\n",
    "    #clusterizando os vetores de acordo com threshold th\n",
    "    clustering = DBSCAN(eps=1-th, min_samples=1, metric='cosine').fit(list_vectors)\n",
    "\n",
    "    #processando os clusters \n",
    "    ents_dic ={}\n",
    "\n",
    "    for i in set(clustering.labels_):\n",
    "        clus_index = np.where(clustering.labels_ == i)[0]\n",
    "        label = []\n",
    "        vec = []\n",
    "        \n",
    "        for c in clus_index:\n",
    "            vec.append(list_ent[c].vector)\n",
    "            label.append(list_ent[c].text)\n",
    "\n",
    "        ents_dic[max(set(label), key=label.count)] = {'labels': list(set(label))} #, 'vector': np.average(vec, axis=0).tolist()}\n",
    "\n",
    "    return ents_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprocessando as listas de entidades para eliminar as entidades duplicadas de cada tese e obtendo o vetor de cada entidade\n",
    "th = 0.90\n",
    "\n",
    "i = 0\n",
    "for key in instances_dic:\n",
    "    for type_ent in instances_dic[key]:\n",
    "        if type_ent != 'lang':\n",
    "            list_ent = instances_dic[key][type_ent]\n",
    "            instances_dic[key][type_ent] = entidades_consolidadas(list_ent, th)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o dicionário com a lista de entidades mencionadas no abstract\n",
    "#with open('data/abstract_NER.json', 'w') as fp:\n",
    "#    json.dump(instances_dic, fp)\n",
    "\n",
    "# Abrindo os dicionários salvos anteriormente\n",
    "with open('data/abstract_NER.json') as fp:\n",
    "    instances_dic = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Clusterizando as entidades em Português\n",
    "\n",
    "per_list = []\n",
    "per_label_list = []\n",
    "per_vector_list = []\n",
    "per_key_list = []\n",
    "\n",
    "for key in instances_dic:\n",
    "    if instances_dic[key]['lang'] == 'pt':\n",
    "        if instances_dic[key]['PER'] != None:\n",
    "            for p in instances_dic[key]['PER']:\n",
    "                per_list.append(p)\n",
    "                per_label_list.append(instances_dic[key]['PER'][p]['labels'])\n",
    "                per_vector_list.append(instances_dic[key]['PER'][p]['vector'])\n",
    "                per_key_list.append(key)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"th =0.98\n",
    "clustering_per = DBSCAN(eps=1-th, min_samples=1, metric='cosine').fit(per_vector_list)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"per_citadas = pd.DataFrame({'teses': per_key_list, 'person': per_list, 'labels' :per_label_list, 'cluster': clustering_per.labels_})\\nper_citadas[per_citadas['cluster'] == 140]\\n#len(set(clustering_per.labels_))\\n#per_label_list[[clustering_per.labels_ == 0]]\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"per_citadas = pd.DataFrame({'teses': per_key_list, 'person': per_list, 'labels' :per_label_list, 'cluster': clustering_per.labels_})\n",
    "per_citadas[per_citadas['cluster'] == 140]\n",
    "#len(set(clustering_per.labels_))\n",
    "#per_label_list[[clustering_per.labels_ == 0]]\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coletando dados da DBPEDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de labels de ORG:  14472\n",
      "Total de labels de PER:  12564\n",
      "Total de labels de LOC:  14126\n"
     ]
    }
   ],
   "source": [
    "# Coletando todas as labels das entidades extraídas dos abstracts\n",
    "\n",
    "people_labels = []\n",
    "local_labels = []\n",
    "org_labels = []\n",
    "\n",
    "for key in instances_dic:\n",
    "    for type_ent in instances_dic[key]:\n",
    "\n",
    "        list_ent = instances_dic[key][type_ent]\n",
    "        if list_ent != None:\n",
    "            for ent in list_ent:\n",
    "                if type_ent == 'PER':\n",
    "                    people_labels = people_labels + list_ent[ent]['labels']\n",
    "                if type_ent == 'LOC':\n",
    "                    local_labels = local_labels + list_ent[ent]['labels']\n",
    "                if type_ent == 'ORG':\n",
    "                    org_labels = org_labels + list_ent[ent]['labels']\n",
    "\n",
    "people_labels = list(set(people_labels))\n",
    "local_labels = list(set(local_labels))\n",
    "org_labels = list(set(org_labels))\n",
    "\n",
    "print('Total de labels de ORG: ', len(org_labels))\n",
    "print('Total de labels de PER: ', len(people_labels))\n",
    "print('Total de labels de LOC: ', len(local_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para buscar as labels na DBPEDIA. A funcao retorna as URI a abstracts de entidades que estao registradas na DBPEDIA.\n",
    "# sc_th é o threshold do score de busca para uma URI da DBPEDIA ser retornada.\n",
    "def DBPEDIA_search(labels, classe, sc_th):\n",
    "\n",
    "    dbpedia_ent = []\n",
    "\n",
    "    for ent_bruto in labels:\n",
    "        # processando as labels para ser buscada na DBPEDIA\n",
    "        ent = re.sub('[^a-zA-Z0-9_ ]', '', unidecode(ent_bruto)).strip()\n",
    "        ent = ent.replace(' and ', ' ').replace(' or ', ' ')\n",
    "        ent = re.sub(\"\\s\\s+\" , \" \", ent)\n",
    "        \n",
    "        ent_vector = \"'\" + ent.replace(\" \", \"', '\") + \"'\"\n",
    "        ent_contains = ent.replace(\" \", \" AND \")\n",
    "\n",
    "        # Sparql query para fazer busca em linguagem natural e retornar os resultados rankeados pelo score ?sc.\n",
    "        query = \"\"\" \n",
    "            define input:ifp \"IFP_OFF\"  \n",
    "            select ?s1 as ?c1, (bif:search_excerpt (bif:vector (\"\"\" + ent_vector + \"\"\"), ?o1)) as ?c2, ?sc, ?rank, ?g, ?abstract \n",
    "            where \n",
    "            { \n",
    "            select ?s1, (?sc * 3e-1) as ?sc, ?o1, (sql:rnk_scale (<LONG::IRI_RANK> (?s1))) as ?rank, ?g, ?abstract \n",
    "            \n",
    "            where  \n",
    "            { \n",
    "                quad map virtrdf:DefaultQuadMap \n",
    "                { \n",
    "                graph ?g \n",
    "                { \n",
    "                    ?s1 ?s1textp ?o1 .\n",
    "                    ?o1 bif:contains  '(\"\"\" + ent_contains + \"\"\")'  option (score ?sc)  .\n",
    "                    ?s1 a \"\"\" + classe + \"\"\".\n",
    "                    ?s1 dbo:abstract ?abstract.\n",
    "                    FILTER (lang(?abstract) IN(\"pt\", \"en\"))\n",
    "                }\n",
    "                } \n",
    "            }\n",
    "\n",
    "            order by desc (?sc * 3e-1 + sql:rnk_scale (<LONG::IRI_RANK> (?s1)))  limit 5  offset 0 \n",
    "            } \n",
    "            \"\"\"\n",
    "        # URL da DBPEDIA e request\n",
    "        url = 'http://dbpedia.org/sparql'\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "            data = r.json()\n",
    "\n",
    "            # processando os resultados obtidos\n",
    "            if data['results']['bindings'] != []:\n",
    "\n",
    "                bindings = []\n",
    "                for r in data['results']['bindings']:\n",
    "                    if float(r['sc']['value']) > sc_th:\n",
    "                        bindings.append((ent_bruto, r['sc']['value'], r['c1']['value'], r['abstract']['value']))\n",
    "                \n",
    "                dbpedia_ent = dbpedia_ent + list(set(bindings))\n",
    "        except:\n",
    "            print('Erro ao buscar a label: ', ent_bruto)\n",
    "    return dbpedia_ent  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcão que recebe uma lista de labels de uma determinada classe, \n",
    "# busca essas labels na DBPEDIA e salva as tuplas com as labels e abstracts no diretório desejado.\n",
    "\n",
    "def coletando_dbpedia_tuplas(labels, classe, path):\n",
    "    # Lista recebe as tuplas\n",
    "    db_tupla = []\n",
    "    #Bach em que as tuplas serão salvas\n",
    "    step = 100\n",
    "    for n in range(0, len(labels), step):\n",
    "        # Busca as labels usando a funcão DBPEDIA_search\n",
    "        ex_DB = DBPEDIA_search(labels[n:n+step], classe, 3.0)\n",
    "        \n",
    "        for ex in ex_DB:\n",
    "            db_tupla.append((ex[2], ex[3]))\n",
    "        # Elimina as tuplas repetidas\n",
    "        db_tupla = list(set(db_tupla))\n",
    "\n",
    "        # Salvando o dicionário com a lista de entidades mencionadas no abstract\n",
    "        with open(path, 'w') as fp:\n",
    "            json.dump(db_tupla, fp)\n",
    "        print(n+step, '- Total de tuplas: ', len(db_tupla))\n",
    "\n",
    "    return db_tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coletando_dbpedia_tuplas(people_labels, 'dbo:Person', 'data/DBPEDIA_people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coletando_dbpedia_tuplas(local_labels, 'dbo:Place', 'data/DBPEDIA_locals.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coletando_dbpedia_tuplas(org_labels, 'dbo:Organisation', 'data/DBPEDIA_orgs.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando as triplas e carregando no knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixos\n",
    "prefixos = \"\"\" @prefix ns: <http://www.w3.org/2003/06/sw-vocab-status/ns#> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix xml: <http://www.w3.org/XML/1998/namespace> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix bibo: <http://purl.org/ontology/bibo/> .\n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\n",
    "@prefix vann: <http://purl.org/vocab/vann/> .\n",
    "@prefix event: <http://purl.org/NET/c4dm/event.owl#> .\n",
    "@prefix prism: <http://prismstandard.org/namespaces/1.2/basic/> .\n",
    "@prefix terms: <http://purl.org/dc/terms/> .\n",
    "@prefix schema: <http://schemas.talis.com/2005/address/schema#> .\n",
    "@prefix status: <http://purl.org/ontology/bibo/status/> .\n",
    "@prefix degrees: <http://purl.org/ontology/bibo/degrees/> .\n",
    "@prefix stardog: <tag:stardog:api:> .\n",
    "@base <http://www.w3.org/2002/07/owl#> .\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcão que recebe os prefixos e triplas e as carrega à base de dados\n",
    "def add_triplas_to_stardog(prefixos, triplas):\n",
    "\n",
    "    # Incluindo prefixos às triplas\n",
    "    triplas = prefixos + \" \" + triplas\n",
    "\n",
    "    ### Connect to the Stardog database\n",
    "    database_name = 'IndigenousSlavery'\n",
    "    conn = stardog.Connection(database_name, **connection_details)\n",
    "\n",
    "    conn.begin()\n",
    "    conn.add(stardog.content.Raw(triplas, 'text/turtle'))\n",
    "    conn.commit() # commit the transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo os dicionários salvos anteriormente\n",
    "with open('data/DBPEDIA_people.json') as fp:\n",
    "    dbpedia_people = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando pessoas\n",
    "triplas = \"\"\" \"\"\"\n",
    "\n",
    "for resource in dbpedia_people:\n",
    "    tripla = \"\"\"\n",
    "    <\"\"\" + resource[0].replace('/resource/', '/page/') + \"\"\"> rdf:type bibo:dbpedia.\n",
    "    <\"\"\" + resource[0] + \"\"\"> rdf:type foaf:Person.\n",
    "    <\"\"\" + resource[0].replace('/resource/', '/page/') + \"\"\"> terms:subject <\"\"\" + resource[0] + \"\"\">. \n",
    "    <\"\"\" + resource[0].replace('/resource/', '/page/') + \"\"\"> bibo:abstract '\"\"\" + str(resource[1]).replace(\"'\",\"\").replace(u'\\\\', u' ') + \"\"\"'@\"\"\" + detect(resource[1]) + \"\"\".\n",
    "    \"\"\"\n",
    "    triplas = triplas + \" \" + tripla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_triplas_to_stardog(prefixos, triplas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo os dicionários salvos anteriormente\n",
    "with open('data/DBPEDIA_locals.json') as fp:\n",
    "    dbpedia_locals = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando locais\n",
    "triplas = \"\"\" \"\"\"\n",
    "\n",
    "for resource in dbpedia_locals:\n",
    "    tripla = \"\"\"\n",
    "    <\"\"\" + resource[0].replace('/resource/', '/page/') + \"\"\"> rdf:type bibo:dbpedia.\n",
    "    <\"\"\" + resource[0] + \"\"\"> rdf:type <https://schema.org/Place>.\n",
    "    <\"\"\" + resource[0].replace('/resource/', '/page/') + \"\"\"> terms:subject <\"\"\" + resource[0] + \"\"\">. \n",
    "    <\"\"\" + resource[0].replace('/resource/', '/page/') + \"\"\"> bibo:abstract '\"\"\" + str(resource[1]).replace(\"'\",\"\").replace(u'\\\\', u' ') + \"\"\"'@\"\"\" + detect(resource[1]) + \"\"\".\n",
    "    \"\"\"\n",
    "    triplas = triplas + \" \" + tripla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_triplas_to_stardog(prefixos, triplas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo os dicionários salvos anteriormente\n",
    "with open('data/DBPEDIA_orgs.json') as fp:\n",
    "    dbpedia_orgs = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando organizacões\n",
    "triplas = \"\"\" \"\"\"\n",
    "\n",
    "for resource in dbpedia_orgs:\n",
    "    tripla = \"\"\"\n",
    "    <\"\"\" + resource[0].replace('/resource/', '/page/') + \"\"\"> rdf:type bibo:dbpedia.\n",
    "    <\"\"\" + resource[0] + \"\"\"> rdf:type foaf:Organization.\n",
    "    <\"\"\" + resource[0].replace('/resource/', '/page/') + \"\"\"> terms:subject <\"\"\" + resource[0] + \"\"\">. \n",
    "    <\"\"\" + resource[0].replace('/resource/', '/page/') + \"\"\"> bibo:abstract '\"\"\" + str(resource[1]).replace(\"'\",\"\").replace(u'\\\\', u' ') + \"\"\"'@\"\"\" + detect(resource[1]) + \"\"\".\n",
    "    \"\"\"\n",
    "    triplas = triplas + \" \" + tripla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_triplas_to_stardog(prefixos, triplas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-IndigSlave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
